# Downloaded Papers

1. [Belief in the Machine: Investigating Epistemological Blind Spots of Language Models](2410.21195_belief_in_the_machine.pdf)
   - Authors: Mirac Suzgun et al.
   - Year: 2024 (arXiv:2410.21195)
   - Focus: KaBLE benchmark probing factual vs belief reasoning, showing epistemic blind spots.

2. [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](2305.04388_cot_unfaithful_explanations.pdf)
   - Authors: Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman
   - Year: 2023 (arXiv:2305.04388)
   - Focus: CoT explanations can be biased and misrepresent model decision bases.

3. [BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief](2109.14723_beliefbank.pdf)
   - Authors: Nora Kassner et al.
   - Year: 2021 (arXiv:2109.14723)
   - Focus: External belief memory with SAT-based consistency to reduce answer shifts.

4. [Calibrated Language Models Must Hallucinate](2311.14648_calibrated_lms_hallucinate.pdf)
   - Authors: Adam Tauman Kalai, Santosh Vempala
   - Year: 2023 (arXiv:2311.14648)
   - Focus: Theoretical lower bounds linking calibration to hallucination rates.

5. [TruthfulQA: Measuring How Models Mimic Human Falsehoods](2109.07958_truthfulqa.pdf)
   - Authors: Stephanie Lin, Jacob Hilton, Owain Evans
   - Year: 2021 (arXiv:2109.07958)
   - Focus: Benchmark for truthful vs sycophantic/hallucinated generations.

6. [From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning](2409.01658_sycophancy_pinpoint_tuning.pdf)
   - Authors: Wei Chen et al.
   - Year: 2024 (arXiv:2409.01658)
   - Focus: Pinpoint tuning to mitigate sycophancy without harming capabilities.
